<!DOCTYPE html>
<html>
<head>
    <title>Trokens: Semantic-Aware Relational Trajectory Tokens
      for Few-Shot Action Recognition</title>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet"> -->

  <!-- <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./static/images/favicon.jpg">



  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>


<body>
    <section class="hero">
      <div class="hero-body no-bottom-padding">
        <div class="container">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Trokens: Semantic-Aware Relational Trajectory Tokens
                for Few-Shot Action Recognition</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a target="_blank" href="https://www.cs.umd.edu/~pulkit/">Pulkit Kumar</a><sup>*,1</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://shuaiyihuang.github.io/">Shuaiyi Huang</a><sup>*,1</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://www.cs.umd.edu/~mwalmer/">Mathew Walmer</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://rssaketh.github.io/">Saketh Rambhatla</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
                  
                  <br /><sup>1</sup>University of Maryland&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>GenAI, Meta
                  <br>&ast; Equal contribution.   
                </span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/2406.00439"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
  
                  <!-- arXiv Link. -->
                  <span class="link-block">
                    <a target="_blank" href="./static/files/Trokens_ICCV2025.pdf"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>ArXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/trokens-iccv25"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span> Code (Coming Soon)</span>
                      </a>
                    </span>
                    <!-- Dataset Link. -->
                    <!-- <span class="link-block">
                      <a href="https://huggingface.co/collections/furonghuang-lab/tracevla-677d98483d23e9cec903d076"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <img src="static/images/hf_icon.svg" />
                        </span>
                        <span>Models</span>
                        </a>
                      </span> -->

                  <!-- <span class="link-block">
                    <a target="_blank" href="https://x.com/cheryyun_l/status/1877746315297230874"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span> -->
                </div>
    
              </div>
              </div>
    
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


<section class="section body" style="padding-top: 0;">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 0px; margin-top: 0;">Motivations</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/imgs/teaser.jpg" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px">While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist. <strong>(a) How can we develop an effective sampling strategy of query points for tracking that balances coverage and efficiency? </strong> Our semantic-aware points adapts better to object scale and semantic relevance while existing methods w/ grid sampling missed small objects w/ important motion (e.g., knife). <strong>(b) How can we explicitly model and utilize the motion patterns captured in point trajectories? </strong> We explicitly model relational motions within a trajectory and across trajectories.</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-two-thirds">
        <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
            <h2 class="title is-3">Abstract</h2>
          </div>
      </div>
    
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds ">
          <div class="content has-text-justified">
            Video understanding requires effective modeling of both
            motion and appearance information, particularly for fewshot
            action recognition. While recent advances in point
            tracking have been shown to improve few-shot action recognition,
            two fundamental challenges persist: selecting informative
            points to track and effectively modeling their
            motion patterns. We present <strong>Trokens</strong>, a novel approach
            that transforms trajectory points into semantic-aware relational
            tokens for action recognition. First, we introduce a
            <strong>semantic-aware sampling strategy</strong> to adaptively distribute
            tracking points based on object scale and semantic relevance.
            Second, we develop <strong>a motion modeling framework</strong>
            that captures both intra-trajectory dynamics through
            the Histogram of Oriented Displacements (HoD) and intertrajectory
            relationships to model complex action patterns.
            Our approach effectively combines these trajectory tokens
            with semantic features to enhance appearance features with
            motion information, achieving state-of-the-art performance
            across six diverse few-shot action recognition benchmarks:
            Something-Something-V2 (both full and small splits), Kinetics,
            UCF101, HMDB51, and FineGym.
          </div>
        </div>
        </div>
      </div>
      </div>
    </div>
  </section>

  


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3" style="text-align: center; padding-bottom: 10px; margin-top: 15px;">Method</h2>

        <div align="center">
          <img src="./static/imgs/overview.jpg" alt="Image description" width="100%">
        </div>  
        <br> 
        <h2 class="subtitle">
          <strong>Trokens</strong> transforms trajectory points into semantic-aware relational
          tokens for action recognition.<br>
          <span style="color: #838585;">(A)</span> Given an input video, we extract appearance tokens using DINOv2. <span style="color: #007bff;">(B)</span> We then cluster these tokens and
sample semantic-aware points in the initial frame, which are tracked using Co-tracker [25] to obtain point trajectories. <span style="color: #ed7e0e;">(C)</span> We compute
intra- and inter-motion features, reorder appearance tokens via token alignment [28], and fuse them with motion features via element-wise
addition to form semantic-aware relational trajectory tokens. <span style="color: #9413ea;">(D)</span> Finally, we input these tokens into a Decoupled Space-Time Transformer
for few-shot action classification.
        </h2>
        
      </div>
  
    </div>
  </section>


  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Qualitative Results</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          
          <div class="columns is-centered has-text-centered">
          
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/imgs/qual3.jpg" alt="Image description" width="100%">
                  </div>
                </div>
              <br>
              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px">Visualization of action trajectory similarities across four
                      classes, where our <strong>semantic-based sampling</strong> enables object-focused
                      trajectories. Each quadrant demonstrates intra-class motion consistency
                      while maintaining inter-class discriminative features.</p>
              </div>
            </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
    <br><br><br><br>

    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Qualitative Results</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
            
            <div class="columns is-vcentered interpolation-panel">
              <div class="column has-text-centered">
                <h5>Unfolding Something</h5>
                <div style="border: 2px dashed #ccc; border-radius: 10px; padding: 15px; margin: 5px;">
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/unfolding_something/105791.gif" alt="Example 1" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/unfolding_something/51842.gif" alt="Example 2" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block;">
                    <img src="./static/imgs/unfolding_something/75931.gif" alt="Example 3" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                </div>
              </div>
              <div class="column has-text-centered">
                <h5>Twisting Something</h5>
                <div style="border: 2px dashed #ccc; border-radius: 10px; padding: 15px; margin: 5px;">
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/twisting_something/131581.gif" alt="Example 1" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/twisting_something/164500.gif" alt="Example 2" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block;">
                    <img src="./static/imgs/twisting_something/59079.gif" alt="Example 3" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                </div>
              </div>
              <div class="column has-text-centered">
                <h5>Putting Sth Next to Sth</h5>
                <div style="border: 2px dashed #ccc; border-radius: 10px; padding: 15px; margin: 5px;">
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/putting_sth_next_to_sth/12945.gif" alt="Example 1" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/putting_sth_next_to_sth/153209.gif" alt="Example 2" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block;">
                    <img src="./static/imgs/putting_sth_next_to_sth/196950.gif" alt="Example 3" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                </div>
              </div>
              <div class="column has-text-centered">
                <h5>Poking a Hole into Sth</h5>
                <div style="border: 2px dashed #ccc; border-radius: 10px; padding: 15px; margin: 5px;">
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/poking_a_hole_into_sth/114459.gif" alt="Example 1" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block; margin-bottom: 10px;">
                    <img src="./static/imgs/poking_a_hole_into_sth/29735.gif" alt="Example 2" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                  <div style="width: 100%; display: inline-block;">
                    <img src="./static/imgs/poking_a_hole_into_sth/3441.gif" alt="Example 3" style="width: 200px; height: 200px; object-fit: cover;">
                  </div>
                </div>
              </div>
            </div>
            <div class="columns is-centered">
              <div class="column">
                  <p style="text-align: left;font-size: 18px">More qualitative results of our semantic-aware point trajectories. For each action class, we randomly selected videos and overlaid them with our extracted semantic-aware point trajectories. Our method successfully focuses on action-relevant objects, even when they are small. We observe that trajectories from the same action class follow very similar motion patterns.</p>
              </div>
            </div>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

 
  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Quantitative Results</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/imgs/quantitative_fig.png" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px"> <strong>Trokens</strong> achieves state-of-the-art few-shot action recognition performance across 1, 3, and 5-shot settings on SSV2, Kinetics, UCF-101, HMDB-51, and FineGym datasets, outperforming all contemporary methods.  </p>
                </div>
              </div>
              <div class="row">
                <div class="col text-center" style="margin-left: -30px;">
                  <img src="./static/imgs/num_points_ablation.png" alt="Image description" width="80%">
                </div>
              </div>
              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px">Few-shot accuracy comparison on SSV2 Small and SSV2 Full (1-shot setting) by varying the number of points. <strong>Trokens</strong> with just 32 points outperforms TATs [28] using 256 points, demonstrating that semantic-aware sampling captures more informative motion information with significantly fewer points.</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Qualitative Results</h2>
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/imgs/qual3.jpg" alt="Image description" width="100%">
                  </div>
                </div>
              <br>
              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px">Visualization of action trajectory similarities across four
                      classes, where semantic-based sampling enables object-focused
                      trajectories. Each quadrant demonstrates intra-class motion consistency
                      while maintaining inter-class discriminative features.</p>
              </div>
            </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{kumar2025trokens,
        title={Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition},
        author={Kumar, Pulkit and Huang, Shuaiyi and Walmer, Mathew and Rambhatla, Saketh and Shrivastava, Abhinav},
        journal={arXiv preprint},
        year={2025}
      }</code></pre>
    </div>
  </section>
     

  <br><br><br>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-8">
          <div class="content">
            <a style="color:hsla(39, 82%, 55%, 0.862)" href="#top"><i class="fa fa-arrow-up"></i><br/>Return to top</a>
            <p>
              Website from <a style="color:hsla(39, 82%, 55%, 0.862)" href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under <a style="color:hsla(39, 82%, 55%, 0.862)"
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

<script>
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.getElementsByTagName('video');
        for (let video of videos) {
            video.playbackRate = 2.0;
        }
    });
</script>

</body>
</html>


